{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26a2316-27f6-4d51-8b87-0a166ba29377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q1. What is the relationship between polynomial functions and kernel functions in machine learning\\nalgorithms?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1d9249-95cf-4e8d-b70e-f69f16ef48d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In machine learning algorithms, polynomial functions and kernel functions are related through their role in transforming data:\\n\\n- **Polynomial Functions**: These are mathematical expressions where features are raised to various powers. For example, a quadratic polynomial involves terms like \\\\( x^2 \\\\) and \\\\( xy \\\\).\\n\\n- **Kernel Functions**: In machine learning, particularly in SVMs, kernel functions allow algorithms to operate in higher-dimensional spaces without explicitly computing the transformed features. A polynomial kernel, for instance, computes the inner product of features in a high-dimensional space corresponding to a polynomial function.\\n\\n**Relationship**: A polynomial kernel function is a type of kernel function that computes the dot product in a space where features are transformed into polynomial terms. This allows SVMs to learn complex boundaries by implicitly mapping the data into higher dimensions, akin to using polynomial features.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''In machine learning algorithms, polynomial functions and kernel functions are related through their role in transforming data:\n",
    "\n",
    "- **Polynomial Functions**: These are mathematical expressions where features are raised to various powers. For example, a quadratic polynomial involves terms like \\( x^2 \\) and \\( xy \\).\n",
    "\n",
    "- **Kernel Functions**: In machine learning, particularly in SVMs, kernel functions allow algorithms to operate in higher-dimensional spaces without explicitly computing the transformed features. A polynomial kernel, for instance, computes the inner product of features in a high-dimensional space corresponding to a polynomial function.\n",
    "\n",
    "**Relationship**: A polynomial kernel function is a type of kernel function that computes the dot product in a space where features are transformed into polynomial terms. This allows SVMs to learn complex boundaries by implicitly mapping the data into higher dimensions, akin to using polynomial features.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb37580-2270-4634-8e84-c6706382dda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06745c75-777d-4eb7-9029-70ed51e7e044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Create SVM model with polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3, C=1.0, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "svm_poly.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d62869-229d-4083-a35e-aef1c4b1aa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbbefbfc-ccd7-4957-a4f8-cf5f14396771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Increasing the value of epsilon in Support Vector Regression (SVR) generally reduces the number of support vectors. A larger epsilon allows for a wider margin of tolerance, meaning more data points are within the epsilon-insensitive zone and do not contribute to the model as support vectors. Consequently, the model is less sensitive to individual data points, leading to fewer support vectors.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Increasing the value of epsilon in Support Vector Regression (SVR) generally reduces the number of support vectors. A larger epsilon allows for a wider margin of tolerance, meaning more data points are within the epsilon-insensitive zone and do not contribute to the model as support vectors. Consequently, the model is less sensitive to individual data points, leading to fewer support vectors.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc11ed8-6a0e-4d95-9c67-e857dc66e3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\\naffect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\\nand provide examples of when you might want to increase or decrease its value?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f16438-0eda-4c60-97c0-3c9f098a42ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Parameters Affecting SVR Performance\\n\\n1. **Kernel Function**:\\n   - **Role**: Determines the shape of the decision boundary.\\n   - **Examples**:\\n     - **Linear Kernel**: Best for linear relationships.\\n     - **Polynomial Kernel**: Captures polynomial relationships; increase degree for more complexity.\\n     - **RBF Kernel**: Useful for capturing non-linear relationships; controlled by gamma.\\n   - **When to Adjust**:\\n     - Use linear for straightforward cases.\\n     - Use polynomial or RBF for complex, non-linear patterns.\\n\\n2. **C Parameter**:\\n   - **Role**: Controls the trade-off between maximizing margin and minimizing classification error.\\n   - **Examples**:\\n     - **High C**: Reduces the margin but fits the training data more closely, which may lead to overfitting.\\n     - **Low C**: Increases the margin, allowing some errors, which may improve generalization.\\n   - **When to Adjust**:\\n     - Increase C for a stricter fit (if overfitting is not an issue).\\n     - Decrease C for a more generalized model (if the model is overfitting).\\n\\n3. **Epsilon Parameter**:\\n   - **Role**: Defines the margin of tolerance where no penalty is given for errors.\\n   - **Examples**:\\n     - **Large Epsilon**: Allows a wider margin of tolerance, leading to fewer support vectors and a smoother model.\\n     - **Small Epsilon**: Requires stricter adherence to the training data, potentially resulting in more support vectors and a more complex model.\\n   - **When to Adjust**:\\n     - Increase epsilon for a smoother model (if noise is present).\\n     - Decrease epsilon for a more precise fit (if precision is crucial).\\n\\n4. **Gamma Parameter** (for RBF Kernel):\\n   - **Role**: Controls the influence of a single training example; affects the shape of the decision boundary.\\n   - **Examples**:\\n     - **High Gamma**: Creates a more complex model with tighter fit to the training data, potentially leading to overfitting.\\n     - **Low Gamma**: Results in a smoother decision boundary, potentially underfitting the model.\\n   - **When to Adjust**:\\n     - Increase gamma for a more complex model (if training data has intricate patterns).\\n     - Decrease gamma for a simpler model (if the data is noisy).\\n\\nIn summary, tuning these parameters affects the balance between fitting the training data and generalizing to new data. Adjusting them depends on the specific problem and data characteristics.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''### Parameters Affecting SVR Performance\n",
    "\n",
    "1. **Kernel Function**:\n",
    "   - **Role**: Determines the shape of the decision boundary.\n",
    "   - **Examples**:\n",
    "     - **Linear Kernel**: Best for linear relationships.\n",
    "     - **Polynomial Kernel**: Captures polynomial relationships; increase degree for more complexity.\n",
    "     - **RBF Kernel**: Useful for capturing non-linear relationships; controlled by gamma.\n",
    "   - **When to Adjust**:\n",
    "     - Use linear for straightforward cases.\n",
    "     - Use polynomial or RBF for complex, non-linear patterns.\n",
    "\n",
    "2. **C Parameter**:\n",
    "   - **Role**: Controls the trade-off between maximizing margin and minimizing classification error.\n",
    "   - **Examples**:\n",
    "     - **High C**: Reduces the margin but fits the training data more closely, which may lead to overfitting.\n",
    "     - **Low C**: Increases the margin, allowing some errors, which may improve generalization.\n",
    "   - **When to Adjust**:\n",
    "     - Increase C for a stricter fit (if overfitting is not an issue).\n",
    "     - Decrease C for a more generalized model (if the model is overfitting).\n",
    "\n",
    "3. **Epsilon Parameter**:\n",
    "   - **Role**: Defines the margin of tolerance where no penalty is given for errors.\n",
    "   - **Examples**:\n",
    "     - **Large Epsilon**: Allows a wider margin of tolerance, leading to fewer support vectors and a smoother model.\n",
    "     - **Small Epsilon**: Requires stricter adherence to the training data, potentially resulting in more support vectors and a more complex model.\n",
    "   - **When to Adjust**:\n",
    "     - Increase epsilon for a smoother model (if noise is present).\n",
    "     - Decrease epsilon for a more precise fit (if precision is crucial).\n",
    "\n",
    "4. **Gamma Parameter** (for RBF Kernel):\n",
    "   - **Role**: Controls the influence of a single training example; affects the shape of the decision boundary.\n",
    "   - **Examples**:\n",
    "     - **High Gamma**: Creates a more complex model with tighter fit to the training data, potentially leading to overfitting.\n",
    "     - **Low Gamma**: Results in a smoother decision boundary, potentially underfitting the model.\n",
    "   - **When to Adjust**:\n",
    "     - Increase gamma for a more complex model (if training data has intricate patterns).\n",
    "     - Decrease gamma for a simpler model (if the data is noisy).\n",
    "\n",
    "In summary, tuning these parameters affects the balance between fitting the training data and generalizing to new data. Adjusting them depends on the specific problem and data characteristics.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e705e0b9-d918-4ddc-a496-7d27d2a057fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q5. Assignment:\\nL Import the necessary libraries and load the dataseg\\nL Split the dataset into training and testing setZ\\nL Preprocess the data using any technique of your choice (e.g. scaling, normalizationK\\nL Create an instance of the SVC classifier and train it on the training datW\\nL Use the trained classifier to predict the labels of the testing datW\\nL Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\\nprecision, recall, F1-scoreK\\nL Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\\nimprove its performanc_\\nL Train the tuned classifier on the entire dataseg\\nL Save the trained classifier to a file for future use.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normalizationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L Use the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9490c675-19c7-4e21-891b-85a814ec99ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Precision: 0.98\n",
      "Recall: 0.98\n",
      "F1 Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Create an instance of SVC with a linear kernel\n",
    "svc = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "# Predict on the test set\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b1ab5-29a1-469e-9433-e2578ef040f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
